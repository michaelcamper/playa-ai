// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.5
//   protoc               unknown
// source: riva_asr.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import {
  type CallOptions,
  type ChannelCredentials,
  Client,
  type ClientDuplexStream,
  type ClientOptions,
  type ClientUnaryCall,
  type handleBidiStreamingCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  type Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";
import { AudioEncoding, audioEncodingFromJSON, audioEncodingToJSON } from "./riva_audio";

export const protobufPackage = "nvidia.riva.asr";

/** RecognizeRequest is used for batch processing of a single audio recording. */
export interface RecognizeRequest {
  /** Provides information to recognizer that specifies how to process the request. */
  config?:
    | RecognitionConfig
    | undefined;
  /**
   * The raw audio data to be processed. The audio bytes must be encoded as specified in
   * `RecognitionConfig`.
   */
  audio: Buffer;
}

/**
 * A StreamingRecognizeRequest is used to configure and stream audio content to the
 * Riva ASR Service. The first message sent must include only a StreamingRecognitionConfig.
 * Subsequent messages sent in the stream must contain only raw bytes of the audio
 * to be recognized.
 */
export interface StreamingRecognizeRequest {
  /**
   * Provides information to the recognizer that specifies how to process the
   * request. The first `StreamingRecognizeRequest` message must contain a
   * `streaming_config`  message.
   */
  streamingConfig?:
    | StreamingRecognitionConfig
    | undefined;
  /**
   * The audio data to be recognized. Sequential chunks of audio data are sent
   * in sequential `StreamingRecognizeRequest` messages. The first
   * `StreamingRecognizeRequest` message must not contain `audio` data
   * and all subsequent `StreamingRecognizeRequest` messages must contain
   * `audio` data. The audio bytes must be encoded as specified in
   * `RecognitionConfig`.
   */
  audioContent?: Buffer | undefined;
}

/** Provides information to the recognizer that specifies how to process the request */
export interface RecognitionConfig {
  /**
   * The encoding of the audio data sent in the request.
   *
   * All encodings support only 1 channel (mono) audio.
   */
  encoding: AudioEncoding;
  /**
   * The sample rate in hertz (Hz) of the audio data sent in the
   * `RecognizeRequest` or `StreamingRecognizeRequest` messages.
   *  The Riva server will automatically down-sample/up-sample the audio to match the ASR acoustic model sample rate.
   *  The sample rate value below 8kHz will not produce any meaningful output.
   */
  sampleRateHertz: number;
  /**
   * Required. The language of the supplied audio as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Example: "en-US".
   * Currently only en-US is supported
   */
  languageCode: string;
  /**
   * Maximum number of recognition hypotheses to be returned.
   * Specifically, the maximum number of `SpeechRecognizeAlternative` messages
   * within each `SpeechRecognizeResult`.
   * The server may return fewer than `max_alternatives`.
   * If omitted, will return a maximum of one.
   */
  maxAlternatives: number;
  /**
   * A custom field that enables profanity filtering for the generated transcripts.
   * If set to 'true', the server filters out profanities, replacing all but the initial
   * character in each filtered word with asterisks. For example, "h***".
   * If set to `false` or omitted, profanities will not be filtered out. The default is `false`.
   */
  profanityFilter: boolean;
  /**
   * Array of SpeechContext.
   * A means to provide context to assist the speech recognition. For more
   * information, see SpeechContext section
   */
  speechContexts: SpeechContext[];
  /**
   * The number of channels in the input audio data.
   * ONLY set this for MULTI-CHANNEL recognition.
   * Valid values for LINEAR16 and FLAC are `1`-`8`.
   * Valid values for OGG_OPUS are '1'-'254'.
   * Valid value for MULAW, AMR, AMR_WB and SPEEX_WITH_HEADER_BYTE is only `1`.
   * If `0` or omitted, defaults to one channel (mono).
   * Note: We only recognize the first channel by default.
   * To perform independent recognition on each channel set
   * `enable_separate_recognition_per_channel` to 'true'.
   */
  audioChannelCount: number;
  /**
   * If `true`, the top result includes a list of words and
   * the start and end time offsets (timestamps) for those words. If
   * `false`, no word-level time offset information is returned. The default is
   * `false`.
   */
  enableWordTimeOffsets: boolean;
  /**
   * If 'true', adds punctuation to recognition result hypotheses.
   * The default 'false' value does not add punctuation to result hypotheses.
   */
  enableAutomaticPunctuation: boolean;
  /**
   * This needs to be set to `true` explicitly and `audio_channel_count` > 1
   * to get each channel recognized separately. The recognition result will
   * contain a `channel_tag` field to state which channel that result belongs
   * to. If this is not true, we will only recognize the first channel. The
   * request is billed cumulatively for all channels recognized:
   * `audio_channel_count` multiplied by the length of the audio.
   */
  enableSeparateRecognitionPerChannel: boolean;
  /** Which model to select for the given request. Valid choices: Jasper, Quartznet */
  model: string;
  /**
   * The verbatim_transcripts flag enables or disable inverse text normalization.
   * 'true' returns exactly what was said, with no denormalization.
   * 'false' applies inverse text normalization, also this is the default
   */
  verbatimTranscripts: boolean;
  /**
   * Config to enable speaker diarization and set additional
   * parameters. For non-streaming requests, the diarization results will be
   * provided only in the top alternative of the FINAL SpeechRecognitionResult.
   */
  diarizationConfig?:
    | SpeakerDiarizationConfig
    | undefined;
  /**
   * Custom fields for passing request-level
   * configuration options to plugins used in the
   * model pipeline.
   */
  customConfiguration: { [key: string]: string };
}

export interface RecognitionConfig_CustomConfigurationEntry {
  key: string;
  value: string;
}

/** Provides information to the recognizer that specifies how to process the request */
export interface StreamingRecognitionConfig {
  /** Provides information to the recognizer that specifies how to process the request */
  config?:
    | RecognitionConfig
    | undefined;
  /**
   * If `true`, interim results (tentative hypotheses) may be
   * returned as they become available (these interim results are indicated with
   * the `is_final=false` flag).
   * If `false` or omitted, only `is_final=true` result(s) are returned.
   */
  interimResults: boolean;
}

/** Config to enable speaker diarization. */
export interface SpeakerDiarizationConfig {
  /**
   * If 'true', enables speaker detection for each recognized word in
   * the top alternative of the recognition result using a speaker_tag provided
   * in the WordInfo.
   */
  enableSpeakerDiarization: boolean;
  /**
   * Maximum number of speakers in the conversation. This gives flexibility by
   * allowing the system to automatically determine the correct number of
   * speakers. If not set, the default value is 8.
   */
  maxSpeakerCount: number;
}

/**
 * Provides "hints" to the speech recognizer to favor specific words and phrases
 * in the results.
 */
export interface SpeechContext {
  /**
   * A list of strings containing words and phrases "hints" so that
   * the speech recognition is more likely to recognize them. This can be used
   * to improve the accuracy for specific words and phrases, for example, if
   * specific commands are typically spoken by the user. This can also be used
   * to add additional words to the vocabulary of the recognizer.
   */
  phrases: string[];
  /**
   * Hint Boost. Positive value will increase the probability that a specific
   * phrase will be recognized over other similar sounding phrases. The higher
   * the boost, the higher the chance of false positive recognition as well.
   * Though `boost` can accept a wide range of positive values, most use cases are best served with
   * values between 0 and 20. We recommend using a binary search approach to
   * finding the optimal value for your use case.
   */
  boost: number;
}

/**
 * The only message returned to the client by the `Recognize` method. It
 * contains the result as zero or more sequential `SpeechRecognitionResult`
 * messages.
 */
export interface RecognizeResponse {
  /**
   * Sequential list of transcription results corresponding to
   * sequential portions of audio. Currently only returns one transcript.
   */
  results: SpeechRecognitionResult[];
}

/** A speech recognition result corresponding to the latest transcript */
export interface SpeechRecognitionResult {
  /**
   * May contain one or more recognition hypotheses (up to the
   * maximum specified in `max_alternatives`).
   * These alternatives are ordered in terms of accuracy, with the top (first)
   * alternative being the most probable, as ranked by the recognizer.
   */
  alternatives: SpeechRecognitionAlternative[];
  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For audio_channel_count = N, its output values can range from '1' to 'N'.
   */
  channelTag: number;
  /** Length of audio processed so far in seconds */
  audioProcessed: number;
}

/** Alternative hypotheses (a.k.a. n-best list). */
export interface SpeechRecognitionAlternative {
  /** Transcript text representing the words that the user spoke. */
  transcript: string;
  /**
   * The non-normalized confidence estimate. A higher number
   * indicates an estimated greater likelihood that the recognized words are
   * correct. This field is set only for a non-streaming
   * result or, of a streaming result where `is_final=true`.
   * This field is not guaranteed to be accurate and users should not rely on it
   * to be always provided.
   */
  confidence: number;
  /**
   * A list of word-specific information for each recognized word. Only populated
   * if is_final=true
   */
  words: WordInfo[];
}

/** Word-specific information for recognized words. */
export interface WordInfo {
  /**
   * Time offset relative to the beginning of the audio in ms
   * and corresponding to the start of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   */
  startTime: number;
  /**
   * Time offset relative to the beginning of the audio in ms
   * and corresponding to the end of the spoken word.
   * This field is only set if `enable_word_time_offsets=true` and only
   * in the top hypothesis.
   */
  endTime: number;
  /** The word corresponding to this set of information. */
  word: string;
  /**
   * The non-normalized confidence estimate. A higher number indicates an
   * estimated greater likelihood that the recognized words are correct. This
   * field is not guaranteed to be accurate and users should not rely on it to
   * be always provided. The default of 0.0 is a sentinel value indicating
   * confidence was not set.
   */
  confidence: number;
  /**
   * Output only. A distinct integer value is assigned for every speaker within
   * the audio. This field specifies which one of those speakers was detected to
   * have spoken this word. Value ranges from '1' to diarization_speaker_count.
   * speaker_tag is set if enable_speaker_diarization = 'true' and only in the
   * top alternative.
   */
  speakerTag: number;
}

export interface StreamingRecognizeResponse {
  /**
   * This repeated list contains the latest transcript(s) corresponding to
   * audio currently being processed.
   * Currently one result is returned, where each result can have multiple
   * alternatives
   */
  results: StreamingRecognitionResult[];
}

/**
 * A streaming speech recognition result corresponding to a portion of the audio
 * that is currently being processed.
 */
export interface StreamingRecognitionResult {
  /**
   * May contain one or more recognition hypotheses (up to the
   * maximum specified in `max_alternatives`).
   * These alternatives are ordered in terms of accuracy, with the top (first)
   * alternative being the most probable, as ranked by the recognizer.
   */
  alternatives: SpeechRecognitionAlternative[];
  /**
   * If `false`, this `StreamingRecognitionResult` represents an
   * interim result that may change. If `true`, this is the final time the
   * speech service will return this particular `StreamingRecognitionResult`,
   * the recognizer will not return any further hypotheses for this portion of
   * the transcript and corresponding audio.
   */
  isFinal: boolean;
  /**
   * An estimate of the likelihood that the recognizer will not
   * change its guess about this interim result. Values range from 0.0
   * (completely unstable) to 1.0 (completely stable).
   * This field is only provided for interim results (`is_final=false`).
   * The default of 0.0 is a sentinel value indicating `stability` was not set.
   */
  stability: number;
  /**
   * For multi-channel audio, this is the channel number corresponding to the
   * recognized result for the audio from that channel.
   * For audio_channel_count = N, its output values can range from '1' to 'N'.
   */
  channelTag: number;
  /** Length of audio processed so far in seconds */
  audioProcessed: number;
}

function createBaseRecognizeRequest(): RecognizeRequest {
  return { config: undefined, audio: Buffer.alloc(0) };
}

export const RecognizeRequest: MessageFns<RecognizeRequest> = {
  encode(message: RecognizeRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      RecognitionConfig.encode(message.config, writer.uint32(10).fork()).join();
    }
    if (message.audio.length !== 0) {
      writer.uint32(18).bytes(message.audio);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognizeRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognizeRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.config = RecognitionConfig.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.audio = Buffer.from(reader.bytes());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognizeRequest {
    return {
      config: isSet(object.config) ? RecognitionConfig.fromJSON(object.config) : undefined,
      audio: isSet(object.audio) ? Buffer.from(bytesFromBase64(object.audio)) : Buffer.alloc(0),
    };
  },

  toJSON(message: RecognizeRequest): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = RecognitionConfig.toJSON(message.config);
    }
    if (message.audio.length !== 0) {
      obj.audio = base64FromBytes(message.audio);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognizeRequest>, I>>(base?: I): RecognizeRequest {
    return RecognizeRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognizeRequest>, I>>(object: I): RecognizeRequest {
    const message = createBaseRecognizeRequest();
    message.config = (object.config !== undefined && object.config !== null)
      ? RecognitionConfig.fromPartial(object.config)
      : undefined;
    message.audio = object.audio ?? Buffer.alloc(0);
    return message;
  },
};

function createBaseStreamingRecognizeRequest(): StreamingRecognizeRequest {
  return { streamingConfig: undefined, audioContent: undefined };
}

export const StreamingRecognizeRequest: MessageFns<StreamingRecognizeRequest> = {
  encode(message: StreamingRecognizeRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.streamingConfig !== undefined) {
      StreamingRecognitionConfig.encode(message.streamingConfig, writer.uint32(10).fork()).join();
    }
    if (message.audioContent !== undefined) {
      writer.uint32(18).bytes(message.audioContent);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognizeRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognizeRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.streamingConfig = StreamingRecognitionConfig.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.audioContent = Buffer.from(reader.bytes());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognizeRequest {
    return {
      streamingConfig: isSet(object.streamingConfig)
        ? StreamingRecognitionConfig.fromJSON(object.streamingConfig)
        : undefined,
      audioContent: isSet(object.audioContent) ? Buffer.from(bytesFromBase64(object.audioContent)) : undefined,
    };
  },

  toJSON(message: StreamingRecognizeRequest): unknown {
    const obj: any = {};
    if (message.streamingConfig !== undefined) {
      obj.streamingConfig = StreamingRecognitionConfig.toJSON(message.streamingConfig);
    }
    if (message.audioContent !== undefined) {
      obj.audioContent = base64FromBytes(message.audioContent);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognizeRequest>, I>>(base?: I): StreamingRecognizeRequest {
    return StreamingRecognizeRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognizeRequest>, I>>(object: I): StreamingRecognizeRequest {
    const message = createBaseStreamingRecognizeRequest();
    message.streamingConfig = (object.streamingConfig !== undefined && object.streamingConfig !== null)
      ? StreamingRecognitionConfig.fromPartial(object.streamingConfig)
      : undefined;
    message.audioContent = object.audioContent ?? undefined;
    return message;
  },
};

function createBaseRecognitionConfig(): RecognitionConfig {
  return {
    encoding: 0,
    sampleRateHertz: 0,
    languageCode: "",
    maxAlternatives: 0,
    profanityFilter: false,
    speechContexts: [],
    audioChannelCount: 0,
    enableWordTimeOffsets: false,
    enableAutomaticPunctuation: false,
    enableSeparateRecognitionPerChannel: false,
    model: "",
    verbatimTranscripts: false,
    diarizationConfig: undefined,
    customConfiguration: {},
  };
}

export const RecognitionConfig: MessageFns<RecognitionConfig> = {
  encode(message: RecognitionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.encoding !== 0) {
      writer.uint32(8).int32(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      writer.uint32(16).int32(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    if (message.maxAlternatives !== 0) {
      writer.uint32(32).int32(message.maxAlternatives);
    }
    if (message.profanityFilter !== false) {
      writer.uint32(40).bool(message.profanityFilter);
    }
    for (const v of message.speechContexts) {
      SpeechContext.encode(v!, writer.uint32(50).fork()).join();
    }
    if (message.audioChannelCount !== 0) {
      writer.uint32(56).int32(message.audioChannelCount);
    }
    if (message.enableWordTimeOffsets !== false) {
      writer.uint32(64).bool(message.enableWordTimeOffsets);
    }
    if (message.enableAutomaticPunctuation !== false) {
      writer.uint32(88).bool(message.enableAutomaticPunctuation);
    }
    if (message.enableSeparateRecognitionPerChannel !== false) {
      writer.uint32(96).bool(message.enableSeparateRecognitionPerChannel);
    }
    if (message.model !== "") {
      writer.uint32(106).string(message.model);
    }
    if (message.verbatimTranscripts !== false) {
      writer.uint32(112).bool(message.verbatimTranscripts);
    }
    if (message.diarizationConfig !== undefined) {
      SpeakerDiarizationConfig.encode(message.diarizationConfig, writer.uint32(122).fork()).join();
    }
    Object.entries(message.customConfiguration).forEach(([key, value]) => {
      RecognitionConfig_CustomConfigurationEntry.encode({ key: key as any, value }, writer.uint32(194).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognitionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognitionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.encoding = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.sampleRateHertz = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.maxAlternatives = reader.int32();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.profanityFilter = reader.bool();
          continue;
        }
        case 6: {
          if (tag !== 50) {
            break;
          }

          message.speechContexts.push(SpeechContext.decode(reader, reader.uint32()));
          continue;
        }
        case 7: {
          if (tag !== 56) {
            break;
          }

          message.audioChannelCount = reader.int32();
          continue;
        }
        case 8: {
          if (tag !== 64) {
            break;
          }

          message.enableWordTimeOffsets = reader.bool();
          continue;
        }
        case 11: {
          if (tag !== 88) {
            break;
          }

          message.enableAutomaticPunctuation = reader.bool();
          continue;
        }
        case 12: {
          if (tag !== 96) {
            break;
          }

          message.enableSeparateRecognitionPerChannel = reader.bool();
          continue;
        }
        case 13: {
          if (tag !== 106) {
            break;
          }

          message.model = reader.string();
          continue;
        }
        case 14: {
          if (tag !== 112) {
            break;
          }

          message.verbatimTranscripts = reader.bool();
          continue;
        }
        case 15: {
          if (tag !== 122) {
            break;
          }

          message.diarizationConfig = SpeakerDiarizationConfig.decode(reader, reader.uint32());
          continue;
        }
        case 24: {
          if (tag !== 194) {
            break;
          }

          const entry24 = RecognitionConfig_CustomConfigurationEntry.decode(reader, reader.uint32());
          if (entry24.value !== undefined) {
            message.customConfiguration[entry24.key] = entry24.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognitionConfig {
    return {
      encoding: isSet(object.encoding) ? audioEncodingFromJSON(object.encoding) : 0,
      sampleRateHertz: isSet(object.sampleRateHertz) ? globalThis.Number(object.sampleRateHertz) : 0,
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
      maxAlternatives: isSet(object.maxAlternatives) ? globalThis.Number(object.maxAlternatives) : 0,
      profanityFilter: isSet(object.profanityFilter) ? globalThis.Boolean(object.profanityFilter) : false,
      speechContexts: globalThis.Array.isArray(object?.speechContexts)
        ? object.speechContexts.map((e: any) => SpeechContext.fromJSON(e))
        : [],
      audioChannelCount: isSet(object.audioChannelCount) ? globalThis.Number(object.audioChannelCount) : 0,
      enableWordTimeOffsets: isSet(object.enableWordTimeOffsets)
        ? globalThis.Boolean(object.enableWordTimeOffsets)
        : false,
      enableAutomaticPunctuation: isSet(object.enableAutomaticPunctuation)
        ? globalThis.Boolean(object.enableAutomaticPunctuation)
        : false,
      enableSeparateRecognitionPerChannel: isSet(object.enableSeparateRecognitionPerChannel)
        ? globalThis.Boolean(object.enableSeparateRecognitionPerChannel)
        : false,
      model: isSet(object.model) ? globalThis.String(object.model) : "",
      verbatimTranscripts: isSet(object.verbatimTranscripts) ? globalThis.Boolean(object.verbatimTranscripts) : false,
      diarizationConfig: isSet(object.diarizationConfig)
        ? SpeakerDiarizationConfig.fromJSON(object.diarizationConfig)
        : undefined,
      customConfiguration: isObject(object.customConfiguration)
        ? Object.entries(object.customConfiguration).reduce<{ [key: string]: string }>((acc, [key, value]) => {
          acc[key] = String(value);
          return acc;
        }, {})
        : {},
    };
  },

  toJSON(message: RecognitionConfig): unknown {
    const obj: any = {};
    if (message.encoding !== 0) {
      obj.encoding = audioEncodingToJSON(message.encoding);
    }
    if (message.sampleRateHertz !== 0) {
      obj.sampleRateHertz = Math.round(message.sampleRateHertz);
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    if (message.maxAlternatives !== 0) {
      obj.maxAlternatives = Math.round(message.maxAlternatives);
    }
    if (message.profanityFilter !== false) {
      obj.profanityFilter = message.profanityFilter;
    }
    if (message.speechContexts?.length) {
      obj.speechContexts = message.speechContexts.map((e) => SpeechContext.toJSON(e));
    }
    if (message.audioChannelCount !== 0) {
      obj.audioChannelCount = Math.round(message.audioChannelCount);
    }
    if (message.enableWordTimeOffsets !== false) {
      obj.enableWordTimeOffsets = message.enableWordTimeOffsets;
    }
    if (message.enableAutomaticPunctuation !== false) {
      obj.enableAutomaticPunctuation = message.enableAutomaticPunctuation;
    }
    if (message.enableSeparateRecognitionPerChannel !== false) {
      obj.enableSeparateRecognitionPerChannel = message.enableSeparateRecognitionPerChannel;
    }
    if (message.model !== "") {
      obj.model = message.model;
    }
    if (message.verbatimTranscripts !== false) {
      obj.verbatimTranscripts = message.verbatimTranscripts;
    }
    if (message.diarizationConfig !== undefined) {
      obj.diarizationConfig = SpeakerDiarizationConfig.toJSON(message.diarizationConfig);
    }
    if (message.customConfiguration) {
      const entries = Object.entries(message.customConfiguration);
      if (entries.length > 0) {
        obj.customConfiguration = {};
        entries.forEach(([k, v]) => {
          obj.customConfiguration[k] = v;
        });
      }
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognitionConfig>, I>>(base?: I): RecognitionConfig {
    return RecognitionConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognitionConfig>, I>>(object: I): RecognitionConfig {
    const message = createBaseRecognitionConfig();
    message.encoding = object.encoding ?? 0;
    message.sampleRateHertz = object.sampleRateHertz ?? 0;
    message.languageCode = object.languageCode ?? "";
    message.maxAlternatives = object.maxAlternatives ?? 0;
    message.profanityFilter = object.profanityFilter ?? false;
    message.speechContexts = object.speechContexts?.map((e) => SpeechContext.fromPartial(e)) || [];
    message.audioChannelCount = object.audioChannelCount ?? 0;
    message.enableWordTimeOffsets = object.enableWordTimeOffsets ?? false;
    message.enableAutomaticPunctuation = object.enableAutomaticPunctuation ?? false;
    message.enableSeparateRecognitionPerChannel = object.enableSeparateRecognitionPerChannel ?? false;
    message.model = object.model ?? "";
    message.verbatimTranscripts = object.verbatimTranscripts ?? false;
    message.diarizationConfig = (object.diarizationConfig !== undefined && object.diarizationConfig !== null)
      ? SpeakerDiarizationConfig.fromPartial(object.diarizationConfig)
      : undefined;
    message.customConfiguration = Object.entries(object.customConfiguration ?? {}).reduce<{ [key: string]: string }>(
      (acc, [key, value]) => {
        if (value !== undefined) {
          acc[key] = globalThis.String(value);
        }
        return acc;
      },
      {},
    );
    return message;
  },
};

function createBaseRecognitionConfig_CustomConfigurationEntry(): RecognitionConfig_CustomConfigurationEntry {
  return { key: "", value: "" };
}

export const RecognitionConfig_CustomConfigurationEntry: MessageFns<RecognitionConfig_CustomConfigurationEntry> = {
  encode(message: RecognitionConfig_CustomConfigurationEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognitionConfig_CustomConfigurationEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognitionConfig_CustomConfigurationEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognitionConfig_CustomConfigurationEntry {
    return {
      key: isSet(object.key) ? globalThis.String(object.key) : "",
      value: isSet(object.value) ? globalThis.String(object.value) : "",
    };
  },

  toJSON(message: RecognitionConfig_CustomConfigurationEntry): unknown {
    const obj: any = {};
    if (message.key !== "") {
      obj.key = message.key;
    }
    if (message.value !== "") {
      obj.value = message.value;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognitionConfig_CustomConfigurationEntry>, I>>(
    base?: I,
  ): RecognitionConfig_CustomConfigurationEntry {
    return RecognitionConfig_CustomConfigurationEntry.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognitionConfig_CustomConfigurationEntry>, I>>(
    object: I,
  ): RecognitionConfig_CustomConfigurationEntry {
    const message = createBaseRecognitionConfig_CustomConfigurationEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseStreamingRecognitionConfig(): StreamingRecognitionConfig {
  return { config: undefined, interimResults: false };
}

export const StreamingRecognitionConfig: MessageFns<StreamingRecognitionConfig> = {
  encode(message: StreamingRecognitionConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.config !== undefined) {
      RecognitionConfig.encode(message.config, writer.uint32(10).fork()).join();
    }
    if (message.interimResults !== false) {
      writer.uint32(16).bool(message.interimResults);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognitionConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognitionConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.config = RecognitionConfig.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.interimResults = reader.bool();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognitionConfig {
    return {
      config: isSet(object.config) ? RecognitionConfig.fromJSON(object.config) : undefined,
      interimResults: isSet(object.interimResults) ? globalThis.Boolean(object.interimResults) : false,
    };
  },

  toJSON(message: StreamingRecognitionConfig): unknown {
    const obj: any = {};
    if (message.config !== undefined) {
      obj.config = RecognitionConfig.toJSON(message.config);
    }
    if (message.interimResults !== false) {
      obj.interimResults = message.interimResults;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognitionConfig>, I>>(base?: I): StreamingRecognitionConfig {
    return StreamingRecognitionConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognitionConfig>, I>>(object: I): StreamingRecognitionConfig {
    const message = createBaseStreamingRecognitionConfig();
    message.config = (object.config !== undefined && object.config !== null)
      ? RecognitionConfig.fromPartial(object.config)
      : undefined;
    message.interimResults = object.interimResults ?? false;
    return message;
  },
};

function createBaseSpeakerDiarizationConfig(): SpeakerDiarizationConfig {
  return { enableSpeakerDiarization: false, maxSpeakerCount: 0 };
}

export const SpeakerDiarizationConfig: MessageFns<SpeakerDiarizationConfig> = {
  encode(message: SpeakerDiarizationConfig, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.enableSpeakerDiarization !== false) {
      writer.uint32(8).bool(message.enableSpeakerDiarization);
    }
    if (message.maxSpeakerCount !== 0) {
      writer.uint32(16).int32(message.maxSpeakerCount);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeakerDiarizationConfig {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeakerDiarizationConfig();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.enableSpeakerDiarization = reader.bool();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.maxSpeakerCount = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeakerDiarizationConfig {
    return {
      enableSpeakerDiarization: isSet(object.enableSpeakerDiarization)
        ? globalThis.Boolean(object.enableSpeakerDiarization)
        : false,
      maxSpeakerCount: isSet(object.maxSpeakerCount) ? globalThis.Number(object.maxSpeakerCount) : 0,
    };
  },

  toJSON(message: SpeakerDiarizationConfig): unknown {
    const obj: any = {};
    if (message.enableSpeakerDiarization !== false) {
      obj.enableSpeakerDiarization = message.enableSpeakerDiarization;
    }
    if (message.maxSpeakerCount !== 0) {
      obj.maxSpeakerCount = Math.round(message.maxSpeakerCount);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeakerDiarizationConfig>, I>>(base?: I): SpeakerDiarizationConfig {
    return SpeakerDiarizationConfig.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeakerDiarizationConfig>, I>>(object: I): SpeakerDiarizationConfig {
    const message = createBaseSpeakerDiarizationConfig();
    message.enableSpeakerDiarization = object.enableSpeakerDiarization ?? false;
    message.maxSpeakerCount = object.maxSpeakerCount ?? 0;
    return message;
  },
};

function createBaseSpeechContext(): SpeechContext {
  return { phrases: [], boost: 0 };
}

export const SpeechContext: MessageFns<SpeechContext> = {
  encode(message: SpeechContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.phrases) {
      writer.uint32(10).string(v!);
    }
    if (message.boost !== 0) {
      writer.uint32(37).float(message.boost);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.phrases.push(reader.string());
          continue;
        }
        case 4: {
          if (tag !== 37) {
            break;
          }

          message.boost = reader.float();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechContext {
    return {
      phrases: globalThis.Array.isArray(object?.phrases) ? object.phrases.map((e: any) => globalThis.String(e)) : [],
      boost: isSet(object.boost) ? globalThis.Number(object.boost) : 0,
    };
  },

  toJSON(message: SpeechContext): unknown {
    const obj: any = {};
    if (message.phrases?.length) {
      obj.phrases = message.phrases;
    }
    if (message.boost !== 0) {
      obj.boost = message.boost;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeechContext>, I>>(base?: I): SpeechContext {
    return SpeechContext.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeechContext>, I>>(object: I): SpeechContext {
    const message = createBaseSpeechContext();
    message.phrases = object.phrases?.map((e) => e) || [];
    message.boost = object.boost ?? 0;
    return message;
  },
};

function createBaseRecognizeResponse(): RecognizeResponse {
  return { results: [] };
}

export const RecognizeResponse: MessageFns<RecognizeResponse> = {
  encode(message: RecognizeResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      SpeechRecognitionResult.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RecognizeResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRecognizeResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(SpeechRecognitionResult.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): RecognizeResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => SpeechRecognitionResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: RecognizeResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => SpeechRecognitionResult.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<RecognizeResponse>, I>>(base?: I): RecognizeResponse {
    return RecognizeResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<RecognizeResponse>, I>>(object: I): RecognizeResponse {
    const message = createBaseRecognizeResponse();
    message.results = object.results?.map((e) => SpeechRecognitionResult.fromPartial(e)) || [];
    return message;
  },
};

function createBaseSpeechRecognitionResult(): SpeechRecognitionResult {
  return { alternatives: [], channelTag: 0, audioProcessed: 0 };
}

export const SpeechRecognitionResult: MessageFns<SpeechRecognitionResult> = {
  encode(message: SpeechRecognitionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alternatives) {
      SpeechRecognitionAlternative.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.channelTag !== 0) {
      writer.uint32(16).int32(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      writer.uint32(29).float(message.audioProcessed);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.alternatives.push(SpeechRecognitionAlternative.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.channelTag = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 29) {
            break;
          }

          message.audioProcessed = reader.float();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionResult {
    return {
      alternatives: globalThis.Array.isArray(object?.alternatives)
        ? object.alternatives.map((e: any) => SpeechRecognitionAlternative.fromJSON(e))
        : [],
      channelTag: isSet(object.channelTag) ? globalThis.Number(object.channelTag) : 0,
      audioProcessed: isSet(object.audioProcessed) ? globalThis.Number(object.audioProcessed) : 0,
    };
  },

  toJSON(message: SpeechRecognitionResult): unknown {
    const obj: any = {};
    if (message.alternatives?.length) {
      obj.alternatives = message.alternatives.map((e) => SpeechRecognitionAlternative.toJSON(e));
    }
    if (message.channelTag !== 0) {
      obj.channelTag = Math.round(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      obj.audioProcessed = message.audioProcessed;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeechRecognitionResult>, I>>(base?: I): SpeechRecognitionResult {
    return SpeechRecognitionResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeechRecognitionResult>, I>>(object: I): SpeechRecognitionResult {
    const message = createBaseSpeechRecognitionResult();
    message.alternatives = object.alternatives?.map((e) => SpeechRecognitionAlternative.fromPartial(e)) || [];
    message.channelTag = object.channelTag ?? 0;
    message.audioProcessed = object.audioProcessed ?? 0;
    return message;
  },
};

function createBaseSpeechRecognitionAlternative(): SpeechRecognitionAlternative {
  return { transcript: "", confidence: 0, words: [] };
}

export const SpeechRecognitionAlternative: MessageFns<SpeechRecognitionAlternative> = {
  encode(message: SpeechRecognitionAlternative, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.transcript !== "") {
      writer.uint32(10).string(message.transcript);
    }
    if (message.confidence !== 0) {
      writer.uint32(21).float(message.confidence);
    }
    for (const v of message.words) {
      WordInfo.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): SpeechRecognitionAlternative {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpeechRecognitionAlternative();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.transcript = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 21) {
            break;
          }

          message.confidence = reader.float();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.words.push(WordInfo.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): SpeechRecognitionAlternative {
    return {
      transcript: isSet(object.transcript) ? globalThis.String(object.transcript) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      words: globalThis.Array.isArray(object?.words) ? object.words.map((e: any) => WordInfo.fromJSON(e)) : [],
    };
  },

  toJSON(message: SpeechRecognitionAlternative): unknown {
    const obj: any = {};
    if (message.transcript !== "") {
      obj.transcript = message.transcript;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.words?.length) {
      obj.words = message.words.map((e) => WordInfo.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<SpeechRecognitionAlternative>, I>>(base?: I): SpeechRecognitionAlternative {
    return SpeechRecognitionAlternative.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<SpeechRecognitionAlternative>, I>>(object: I): SpeechRecognitionAlternative {
    const message = createBaseSpeechRecognitionAlternative();
    message.transcript = object.transcript ?? "";
    message.confidence = object.confidence ?? 0;
    message.words = object.words?.map((e) => WordInfo.fromPartial(e)) || [];
    return message;
  },
};

function createBaseWordInfo(): WordInfo {
  return { startTime: 0, endTime: 0, word: "", confidence: 0, speakerTag: 0 };
}

export const WordInfo: MessageFns<WordInfo> = {
  encode(message: WordInfo, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.startTime !== 0) {
      writer.uint32(8).int32(message.startTime);
    }
    if (message.endTime !== 0) {
      writer.uint32(16).int32(message.endTime);
    }
    if (message.word !== "") {
      writer.uint32(26).string(message.word);
    }
    if (message.confidence !== 0) {
      writer.uint32(37).float(message.confidence);
    }
    if (message.speakerTag !== 0) {
      writer.uint32(40).int32(message.speakerTag);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): WordInfo {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseWordInfo();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.startTime = reader.int32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.endTime = reader.int32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.word = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 37) {
            break;
          }

          message.confidence = reader.float();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.speakerTag = reader.int32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): WordInfo {
    return {
      startTime: isSet(object.startTime) ? globalThis.Number(object.startTime) : 0,
      endTime: isSet(object.endTime) ? globalThis.Number(object.endTime) : 0,
      word: isSet(object.word) ? globalThis.String(object.word) : "",
      confidence: isSet(object.confidence) ? globalThis.Number(object.confidence) : 0,
      speakerTag: isSet(object.speakerTag) ? globalThis.Number(object.speakerTag) : 0,
    };
  },

  toJSON(message: WordInfo): unknown {
    const obj: any = {};
    if (message.startTime !== 0) {
      obj.startTime = Math.round(message.startTime);
    }
    if (message.endTime !== 0) {
      obj.endTime = Math.round(message.endTime);
    }
    if (message.word !== "") {
      obj.word = message.word;
    }
    if (message.confidence !== 0) {
      obj.confidence = message.confidence;
    }
    if (message.speakerTag !== 0) {
      obj.speakerTag = Math.round(message.speakerTag);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<WordInfo>, I>>(base?: I): WordInfo {
    return WordInfo.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<WordInfo>, I>>(object: I): WordInfo {
    const message = createBaseWordInfo();
    message.startTime = object.startTime ?? 0;
    message.endTime = object.endTime ?? 0;
    message.word = object.word ?? "";
    message.confidence = object.confidence ?? 0;
    message.speakerTag = object.speakerTag ?? 0;
    return message;
  },
};

function createBaseStreamingRecognizeResponse(): StreamingRecognizeResponse {
  return { results: [] };
}

export const StreamingRecognizeResponse: MessageFns<StreamingRecognizeResponse> = {
  encode(message: StreamingRecognizeResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      StreamingRecognitionResult.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognizeResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognizeResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(StreamingRecognitionResult.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognizeResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => StreamingRecognitionResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: StreamingRecognizeResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => StreamingRecognitionResult.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognizeResponse>, I>>(base?: I): StreamingRecognizeResponse {
    return StreamingRecognizeResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognizeResponse>, I>>(object: I): StreamingRecognizeResponse {
    const message = createBaseStreamingRecognizeResponse();
    message.results = object.results?.map((e) => StreamingRecognitionResult.fromPartial(e)) || [];
    return message;
  },
};

function createBaseStreamingRecognitionResult(): StreamingRecognitionResult {
  return { alternatives: [], isFinal: false, stability: 0, channelTag: 0, audioProcessed: 0 };
}

export const StreamingRecognitionResult: MessageFns<StreamingRecognitionResult> = {
  encode(message: StreamingRecognitionResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.alternatives) {
      SpeechRecognitionAlternative.encode(v!, writer.uint32(10).fork()).join();
    }
    if (message.isFinal !== false) {
      writer.uint32(16).bool(message.isFinal);
    }
    if (message.stability !== 0) {
      writer.uint32(29).float(message.stability);
    }
    if (message.channelTag !== 0) {
      writer.uint32(40).int32(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      writer.uint32(53).float(message.audioProcessed);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): StreamingRecognitionResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseStreamingRecognitionResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.alternatives.push(SpeechRecognitionAlternative.decode(reader, reader.uint32()));
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.isFinal = reader.bool();
          continue;
        }
        case 3: {
          if (tag !== 29) {
            break;
          }

          message.stability = reader.float();
          continue;
        }
        case 5: {
          if (tag !== 40) {
            break;
          }

          message.channelTag = reader.int32();
          continue;
        }
        case 6: {
          if (tag !== 53) {
            break;
          }

          message.audioProcessed = reader.float();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): StreamingRecognitionResult {
    return {
      alternatives: globalThis.Array.isArray(object?.alternatives)
        ? object.alternatives.map((e: any) => SpeechRecognitionAlternative.fromJSON(e))
        : [],
      isFinal: isSet(object.isFinal) ? globalThis.Boolean(object.isFinal) : false,
      stability: isSet(object.stability) ? globalThis.Number(object.stability) : 0,
      channelTag: isSet(object.channelTag) ? globalThis.Number(object.channelTag) : 0,
      audioProcessed: isSet(object.audioProcessed) ? globalThis.Number(object.audioProcessed) : 0,
    };
  },

  toJSON(message: StreamingRecognitionResult): unknown {
    const obj: any = {};
    if (message.alternatives?.length) {
      obj.alternatives = message.alternatives.map((e) => SpeechRecognitionAlternative.toJSON(e));
    }
    if (message.isFinal !== false) {
      obj.isFinal = message.isFinal;
    }
    if (message.stability !== 0) {
      obj.stability = message.stability;
    }
    if (message.channelTag !== 0) {
      obj.channelTag = Math.round(message.channelTag);
    }
    if (message.audioProcessed !== 0) {
      obj.audioProcessed = message.audioProcessed;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<StreamingRecognitionResult>, I>>(base?: I): StreamingRecognitionResult {
    return StreamingRecognitionResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<StreamingRecognitionResult>, I>>(object: I): StreamingRecognitionResult {
    const message = createBaseStreamingRecognitionResult();
    message.alternatives = object.alternatives?.map((e) => SpeechRecognitionAlternative.fromPartial(e)) || [];
    message.isFinal = object.isFinal ?? false;
    message.stability = object.stability ?? 0;
    message.channelTag = object.channelTag ?? 0;
    message.audioProcessed = object.audioProcessed ?? 0;
    return message;
  },
};

/** The RivaSpeechRecognition service provides two mechanisms for converting speech to text. */
export type RivaSpeechRecognitionService = typeof RivaSpeechRecognitionService;
export const RivaSpeechRecognitionService = {
  /**
   * Recognize expects a RecognizeRequest and returns a RecognizeResponse. This request will block
   * until the audio is uploaded, processed, and a transcript is returned.
   */
  recognize: {
    path: "/nvidia.riva.asr.RivaSpeechRecognition/Recognize",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: RecognizeRequest): Buffer => Buffer.from(RecognizeRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): RecognizeRequest => RecognizeRequest.decode(value),
    responseSerialize: (value: RecognizeResponse): Buffer => Buffer.from(RecognizeResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): RecognizeResponse => RecognizeResponse.decode(value),
  },
  /**
   * StreamingRecognize is a non-blocking API call that allows audio data to be fed to the server in
   * chunks as it becomes available. Depending on the configuration in the StreamingRecognizeRequest,
   * intermediate results can be sent back to the client. Recognition ends when the stream is closed
   * by the client.
   */
  streamingRecognize: {
    path: "/nvidia.riva.asr.RivaSpeechRecognition/StreamingRecognize",
    requestStream: true,
    responseStream: true,
    requestSerialize: (value: StreamingRecognizeRequest): Buffer =>
      Buffer.from(StreamingRecognizeRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): StreamingRecognizeRequest => StreamingRecognizeRequest.decode(value),
    responseSerialize: (value: StreamingRecognizeResponse): Buffer =>
      Buffer.from(StreamingRecognizeResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): StreamingRecognizeResponse => StreamingRecognizeResponse.decode(value),
  },
} as const;

export interface RivaSpeechRecognitionServer extends UntypedServiceImplementation {
  /**
   * Recognize expects a RecognizeRequest and returns a RecognizeResponse. This request will block
   * until the audio is uploaded, processed, and a transcript is returned.
   */
  recognize: handleUnaryCall<RecognizeRequest, RecognizeResponse>;
  /**
   * StreamingRecognize is a non-blocking API call that allows audio data to be fed to the server in
   * chunks as it becomes available. Depending on the configuration in the StreamingRecognizeRequest,
   * intermediate results can be sent back to the client. Recognition ends when the stream is closed
   * by the client.
   */
  streamingRecognize: handleBidiStreamingCall<StreamingRecognizeRequest, StreamingRecognizeResponse>;
}

export interface RivaSpeechRecognitionClient extends Client {
  /**
   * Recognize expects a RecognizeRequest and returns a RecognizeResponse. This request will block
   * until the audio is uploaded, processed, and a transcript is returned.
   */
  recognize(
    request: RecognizeRequest,
    callback: (error: ServiceError | null, response: RecognizeResponse) => void,
  ): ClientUnaryCall;
  recognize(
    request: RecognizeRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: RecognizeResponse) => void,
  ): ClientUnaryCall;
  recognize(
    request: RecognizeRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: RecognizeResponse) => void,
  ): ClientUnaryCall;
  /**
   * StreamingRecognize is a non-blocking API call that allows audio data to be fed to the server in
   * chunks as it becomes available. Depending on the configuration in the StreamingRecognizeRequest,
   * intermediate results can be sent back to the client. Recognition ends when the stream is closed
   * by the client.
   */
  streamingRecognize(): ClientDuplexStream<StreamingRecognizeRequest, StreamingRecognizeResponse>;
  streamingRecognize(
    options: Partial<CallOptions>,
  ): ClientDuplexStream<StreamingRecognizeRequest, StreamingRecognizeResponse>;
  streamingRecognize(
    metadata: Metadata,
    options?: Partial<CallOptions>,
  ): ClientDuplexStream<StreamingRecognizeRequest, StreamingRecognizeResponse>;
}

export const RivaSpeechRecognitionClient = makeGenericClientConstructor(
  RivaSpeechRecognitionService,
  "nvidia.riva.asr.RivaSpeechRecognition",
) as unknown as {
  new (address: string, credentials: ChannelCredentials, options?: Partial<ClientOptions>): RivaSpeechRecognitionClient;
  service: typeof RivaSpeechRecognitionService;
  serviceName: string;
};

function bytesFromBase64(b64: string): Uint8Array {
  return Uint8Array.from(globalThis.Buffer.from(b64, "base64"));
}

function base64FromBytes(arr: Uint8Array): string {
  return globalThis.Buffer.from(arr).toString("base64");
}

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function isObject(value: any): boolean {
  return typeof value === "object" && value !== null;
}

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
