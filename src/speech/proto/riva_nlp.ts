// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.7.5
//   protoc               unknown
// source: riva_nlp.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";
import {
  type CallOptions,
  type ChannelCredentials,
  Client,
  type ClientOptions,
  type ClientUnaryCall,
  type handleUnaryCall,
  makeGenericClientConstructor,
  type Metadata,
  type ServiceError,
  type UntypedServiceImplementation,
} from "@grpc/grpc-js";

export const protobufPackage = "nvidia.riva.nlp";

/**
 * NLPModelParams is a metadata message that is included in every request message
 * used by the Core NLP Service and is used to specify model characteristics/requirements
 */
export interface NLPModelParams {
  /** Requested model to use. If specified, this takes preference over language_code. */
  modelName: string;
  /**
   * Specify language of the supplied text as a
   * [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
   * Defaults to "en-US" if not set.
   */
  languageCode: string;
}

/**
 * TextTransformRequest is a request type intended for services like TransformText
 * which take an arbitrary text input
 */
export interface TextTransformRequest {
  /**
   * Each repeated text element is handled independently for handling multiple
   * input strings with a single request
   */
  text: string[];
  /**  */
  topN: number;
  model?: NLPModelParams | undefined;
}

/**
 * TextTransformResponse is returned by the TransformText method. Responses
 * are returned in the same order as they were requested.
 */
export interface TextTransformResponse {
  text: string[];
}

/** TextClassRequest is the input message to the ClassifyText service. */
export interface TextClassRequest {
  /**
   * Each repeated text element is handled independently for handling multiple
   * input strings with a single request
   */
  text: string[];
  /**
   * Return the top N classification results for each input. 0 or 1 will return top class, otherwise N.
   * Note: Current disabled.
   */
  topN: number;
  model?: NLPModelParams | undefined;
}

/** Classification messages return a class name and corresponding score */
export interface Classification {
  className: string;
  score: number;
}

/** Span of a particular result */
export interface Span {
  start: number;
  end: number;
}

/**
 * ClassificationResults contain zero or more Classification messages
 * If the number of Classifications is > 1, top_n > 1 must have been
 * specified.
 */
export interface ClassificationResult {
  labels: Classification[];
}

/** TextClassResponse is the return message from the ClassifyText service. */
export interface TextClassResponse {
  results: ClassificationResult[];
}

/** TokenClassRequest is the input message to the ClassifyText service. */
export interface TokenClassRequest {
  /**
   * Each repeated text element is handled independently for handling multiple
   * input strings with a single request
   */
  text: string[];
  /**
   * Return the top N classification results for each input. 0 or 1 will return top class, otherwise N.
   * Note: Current disabled.
   */
  topN: number;
  model?: NLPModelParams | undefined;
}

/** TokenClassValue is used to correlate an input token with its classification results */
export interface TokenClassValue {
  token: string;
  label: Classification[];
  span: Span[];
}

/**
 * TokenClassSequence is used for returning a sequence of TokenClassValue objects
 * in the original order of input tokens
 */
export interface TokenClassSequence {
  results: TokenClassValue[];
}

/** TokenClassResponse returns a single TokenClassSequence per input request */
export interface TokenClassResponse {
  results: TokenClassSequence[];
}

/**
 * AnalyzeIntentContext is reserved for future use when we may send context back in a
 * a variety of different formats (including raw neural network hidden states)
 */
export interface AnalyzeIntentContext {
}

/**
 * AnalyzeIntentOptions is an optional configuration message to be sent as part of
 * an AnalyzeIntentRequest with query metadata
 */
export interface AnalyzeIntentOptions {
  previousIntent?: string | undefined;
  vectors?:
    | AnalyzeIntentContext
    | undefined;
  /**
   * Optional domain field. Domain must be supported otherwise an error will be returned.
   * If left blank, a domain detector will be run first and then the query routed to the
   * appropriate intent classifier (if it exists)
   */
  domain: string;
  /** Optional language field. Assumed to be "en-US" if not specified. */
  lang: string;
}

/** AnalyzeIntentRequest is the input message for the AnalyzeIntent service */
export interface AnalyzeIntentRequest {
  /** The string to analyze for intent and slots */
  query: string;
  /**
   * Optional configuration for the request, including providing context from previous turns
   * and hardcoding a domain/language
   */
  options?: AnalyzeIntentOptions | undefined;
}

/**
 * AnalyzeIntentResponse is returned by the AnalyzeIntent service, and includes information
 * related to the query's intent, (optionally) slot data, and its domain.
 */
export interface AnalyzeIntentResponse {
  /** Intent classification result, including the label and score */
  intent?:
    | Classification
    | undefined;
  /**
   * List of tokens explicitly marked as filling a slot relevant to the intent, where the
   * tokens may not exactly match the input (based on the recombined values after tokenization)
   */
  slots: TokenClassValue[];
  /**
   * Returns the inferred domain for the query if not hardcoded in the request. In the case where
   * the domain was hardcoded in AnalyzeIntentRequest, the returned domain is an exact match to the
   * request. In the case where no domain matches the query, intent and slots will be unset.
   *
   * DEPRECATED, use Classification domain field.
   */
  domainStr: string;
  /**
   * Returns the inferred domain for the query if not hardcoded in the request. In the case where
   * the domain was hardcoded in AnalyzeIntentRequest, the returned domain is an exact match to the
   * request. In the case where no domain matches the query, intent and slots will be unset.
   */
  domain?: Classification | undefined;
}

/**
 * AnalyzeEntitiesOptions is an optional configuration message to be sent as part of
 * an AnalyzeEntitiesRequest with query metadata
 */
export interface AnalyzeEntitiesOptions {
  /** Optional language field. Assumed to be "en-US" if not specified. */
  lang: string;
}

/** AnalyzeEntitiesRequest is the input message for the AnalyzeEntities service */
export interface AnalyzeEntitiesRequest {
  /** The string to analyze for intent and slots */
  query: string;
  /**
   * Optional configuration for the request, including providing context from previous turns
   * and hardcoding a domain/language
   */
  options?: AnalyzeEntitiesOptions | undefined;
}

export interface NaturalQueryRequest {
  /** The natural language query */
  query: string;
  /** Maximum number of answers to return for the query. Defaults to 1 if not set. */
  topN: number;
  /** Context to search with the above query */
  context: string;
}

export interface NaturalQueryResult {
  /** text which answers the query */
  answer: string;
  /** Score representing confidence in result */
  score: number;
}

export interface NaturalQueryResponse {
  results: NaturalQueryResult[];
}

function createBaseNLPModelParams(): NLPModelParams {
  return { modelName: "", languageCode: "" };
}

export const NLPModelParams: MessageFns<NLPModelParams> = {
  encode(message: NLPModelParams, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.modelName !== "") {
      writer.uint32(10).string(message.modelName);
    }
    if (message.languageCode !== "") {
      writer.uint32(26).string(message.languageCode);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NLPModelParams {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNLPModelParams();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.modelName = reader.string();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.languageCode = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NLPModelParams {
    return {
      modelName: isSet(object.modelName) ? globalThis.String(object.modelName) : "",
      languageCode: isSet(object.languageCode) ? globalThis.String(object.languageCode) : "",
    };
  },

  toJSON(message: NLPModelParams): unknown {
    const obj: any = {};
    if (message.modelName !== "") {
      obj.modelName = message.modelName;
    }
    if (message.languageCode !== "") {
      obj.languageCode = message.languageCode;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<NLPModelParams>, I>>(base?: I): NLPModelParams {
    return NLPModelParams.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<NLPModelParams>, I>>(object: I): NLPModelParams {
    const message = createBaseNLPModelParams();
    message.modelName = object.modelName ?? "";
    message.languageCode = object.languageCode ?? "";
    return message;
  },
};

function createBaseTextTransformRequest(): TextTransformRequest {
  return { text: [], topN: 0, model: undefined };
}

export const TextTransformRequest: MessageFns<TextTransformRequest> = {
  encode(message: TextTransformRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.text) {
      writer.uint32(10).string(v!);
    }
    if (message.topN !== 0) {
      writer.uint32(16).uint32(message.topN);
    }
    if (message.model !== undefined) {
      NLPModelParams.encode(message.model, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextTransformRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextTransformRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.text.push(reader.string());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.topN = reader.uint32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.model = NLPModelParams.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextTransformRequest {
    return {
      text: globalThis.Array.isArray(object?.text) ? object.text.map((e: any) => globalThis.String(e)) : [],
      topN: isSet(object.topN) ? globalThis.Number(object.topN) : 0,
      model: isSet(object.model) ? NLPModelParams.fromJSON(object.model) : undefined,
    };
  },

  toJSON(message: TextTransformRequest): unknown {
    const obj: any = {};
    if (message.text?.length) {
      obj.text = message.text;
    }
    if (message.topN !== 0) {
      obj.topN = Math.round(message.topN);
    }
    if (message.model !== undefined) {
      obj.model = NLPModelParams.toJSON(message.model);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextTransformRequest>, I>>(base?: I): TextTransformRequest {
    return TextTransformRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextTransformRequest>, I>>(object: I): TextTransformRequest {
    const message = createBaseTextTransformRequest();
    message.text = object.text?.map((e) => e) || [];
    message.topN = object.topN ?? 0;
    message.model = (object.model !== undefined && object.model !== null)
      ? NLPModelParams.fromPartial(object.model)
      : undefined;
    return message;
  },
};

function createBaseTextTransformResponse(): TextTransformResponse {
  return { text: [] };
}

export const TextTransformResponse: MessageFns<TextTransformResponse> = {
  encode(message: TextTransformResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.text) {
      writer.uint32(10).string(v!);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextTransformResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextTransformResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.text.push(reader.string());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextTransformResponse {
    return { text: globalThis.Array.isArray(object?.text) ? object.text.map((e: any) => globalThis.String(e)) : [] };
  },

  toJSON(message: TextTransformResponse): unknown {
    const obj: any = {};
    if (message.text?.length) {
      obj.text = message.text;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextTransformResponse>, I>>(base?: I): TextTransformResponse {
    return TextTransformResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextTransformResponse>, I>>(object: I): TextTransformResponse {
    const message = createBaseTextTransformResponse();
    message.text = object.text?.map((e) => e) || [];
    return message;
  },
};

function createBaseTextClassRequest(): TextClassRequest {
  return { text: [], topN: 0, model: undefined };
}

export const TextClassRequest: MessageFns<TextClassRequest> = {
  encode(message: TextClassRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.text) {
      writer.uint32(10).string(v!);
    }
    if (message.topN !== 0) {
      writer.uint32(16).uint32(message.topN);
    }
    if (message.model !== undefined) {
      NLPModelParams.encode(message.model, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextClassRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextClassRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.text.push(reader.string());
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.topN = reader.uint32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.model = NLPModelParams.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextClassRequest {
    return {
      text: globalThis.Array.isArray(object?.text) ? object.text.map((e: any) => globalThis.String(e)) : [],
      topN: isSet(object.topN) ? globalThis.Number(object.topN) : 0,
      model: isSet(object.model) ? NLPModelParams.fromJSON(object.model) : undefined,
    };
  },

  toJSON(message: TextClassRequest): unknown {
    const obj: any = {};
    if (message.text?.length) {
      obj.text = message.text;
    }
    if (message.topN !== 0) {
      obj.topN = Math.round(message.topN);
    }
    if (message.model !== undefined) {
      obj.model = NLPModelParams.toJSON(message.model);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextClassRequest>, I>>(base?: I): TextClassRequest {
    return TextClassRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextClassRequest>, I>>(object: I): TextClassRequest {
    const message = createBaseTextClassRequest();
    message.text = object.text?.map((e) => e) || [];
    message.topN = object.topN ?? 0;
    message.model = (object.model !== undefined && object.model !== null)
      ? NLPModelParams.fromPartial(object.model)
      : undefined;
    return message;
  },
};

function createBaseClassification(): Classification {
  return { className: "", score: 0 };
}

export const Classification: MessageFns<Classification> = {
  encode(message: Classification, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.className !== "") {
      writer.uint32(10).string(message.className);
    }
    if (message.score !== 0) {
      writer.uint32(21).float(message.score);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Classification {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClassification();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.className = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 21) {
            break;
          }

          message.score = reader.float();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Classification {
    return {
      className: isSet(object.className) ? globalThis.String(object.className) : "",
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
    };
  },

  toJSON(message: Classification): unknown {
    const obj: any = {};
    if (message.className !== "") {
      obj.className = message.className;
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Classification>, I>>(base?: I): Classification {
    return Classification.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Classification>, I>>(object: I): Classification {
    const message = createBaseClassification();
    message.className = object.className ?? "";
    message.score = object.score ?? 0;
    return message;
  },
};

function createBaseSpan(): Span {
  return { start: 0, end: 0 };
}

export const Span: MessageFns<Span> = {
  encode(message: Span, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.start !== 0) {
      writer.uint32(8).uint32(message.start);
    }
    if (message.end !== 0) {
      writer.uint32(16).uint32(message.end);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Span {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseSpan();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.start = reader.uint32();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.end = reader.uint32();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): Span {
    return {
      start: isSet(object.start) ? globalThis.Number(object.start) : 0,
      end: isSet(object.end) ? globalThis.Number(object.end) : 0,
    };
  },

  toJSON(message: Span): unknown {
    const obj: any = {};
    if (message.start !== 0) {
      obj.start = Math.round(message.start);
    }
    if (message.end !== 0) {
      obj.end = Math.round(message.end);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<Span>, I>>(base?: I): Span {
    return Span.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<Span>, I>>(object: I): Span {
    const message = createBaseSpan();
    message.start = object.start ?? 0;
    message.end = object.end ?? 0;
    return message;
  },
};

function createBaseClassificationResult(): ClassificationResult {
  return { labels: [] };
}

export const ClassificationResult: MessageFns<ClassificationResult> = {
  encode(message: ClassificationResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.labels) {
      Classification.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ClassificationResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseClassificationResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.labels.push(Classification.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): ClassificationResult {
    return {
      labels: globalThis.Array.isArray(object?.labels) ? object.labels.map((e: any) => Classification.fromJSON(e)) : [],
    };
  },

  toJSON(message: ClassificationResult): unknown {
    const obj: any = {};
    if (message.labels?.length) {
      obj.labels = message.labels.map((e) => Classification.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<ClassificationResult>, I>>(base?: I): ClassificationResult {
    return ClassificationResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<ClassificationResult>, I>>(object: I): ClassificationResult {
    const message = createBaseClassificationResult();
    message.labels = object.labels?.map((e) => Classification.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTextClassResponse(): TextClassResponse {
  return { results: [] };
}

export const TextClassResponse: MessageFns<TextClassResponse> = {
  encode(message: TextClassResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      ClassificationResult.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TextClassResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTextClassResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(ClassificationResult.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TextClassResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => ClassificationResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TextClassResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => ClassificationResult.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TextClassResponse>, I>>(base?: I): TextClassResponse {
    return TextClassResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TextClassResponse>, I>>(object: I): TextClassResponse {
    const message = createBaseTextClassResponse();
    message.results = object.results?.map((e) => ClassificationResult.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTokenClassRequest(): TokenClassRequest {
  return { text: [], topN: 0, model: undefined };
}

export const TokenClassRequest: MessageFns<TokenClassRequest> = {
  encode(message: TokenClassRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.text) {
      writer.uint32(10).string(v!);
    }
    if (message.topN !== 0) {
      writer.uint32(24).uint32(message.topN);
    }
    if (message.model !== undefined) {
      NLPModelParams.encode(message.model, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TokenClassRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTokenClassRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.text.push(reader.string());
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.topN = reader.uint32();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.model = NLPModelParams.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TokenClassRequest {
    return {
      text: globalThis.Array.isArray(object?.text) ? object.text.map((e: any) => globalThis.String(e)) : [],
      topN: isSet(object.topN) ? globalThis.Number(object.topN) : 0,
      model: isSet(object.model) ? NLPModelParams.fromJSON(object.model) : undefined,
    };
  },

  toJSON(message: TokenClassRequest): unknown {
    const obj: any = {};
    if (message.text?.length) {
      obj.text = message.text;
    }
    if (message.topN !== 0) {
      obj.topN = Math.round(message.topN);
    }
    if (message.model !== undefined) {
      obj.model = NLPModelParams.toJSON(message.model);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TokenClassRequest>, I>>(base?: I): TokenClassRequest {
    return TokenClassRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TokenClassRequest>, I>>(object: I): TokenClassRequest {
    const message = createBaseTokenClassRequest();
    message.text = object.text?.map((e) => e) || [];
    message.topN = object.topN ?? 0;
    message.model = (object.model !== undefined && object.model !== null)
      ? NLPModelParams.fromPartial(object.model)
      : undefined;
    return message;
  },
};

function createBaseTokenClassValue(): TokenClassValue {
  return { token: "", label: [], span: [] };
}

export const TokenClassValue: MessageFns<TokenClassValue> = {
  encode(message: TokenClassValue, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.token !== "") {
      writer.uint32(10).string(message.token);
    }
    for (const v of message.label) {
      Classification.encode(v!, writer.uint32(18).fork()).join();
    }
    for (const v of message.span) {
      Span.encode(v!, writer.uint32(26).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TokenClassValue {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTokenClassValue();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.token = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.label.push(Classification.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.span.push(Span.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TokenClassValue {
    return {
      token: isSet(object.token) ? globalThis.String(object.token) : "",
      label: globalThis.Array.isArray(object?.label) ? object.label.map((e: any) => Classification.fromJSON(e)) : [],
      span: globalThis.Array.isArray(object?.span) ? object.span.map((e: any) => Span.fromJSON(e)) : [],
    };
  },

  toJSON(message: TokenClassValue): unknown {
    const obj: any = {};
    if (message.token !== "") {
      obj.token = message.token;
    }
    if (message.label?.length) {
      obj.label = message.label.map((e) => Classification.toJSON(e));
    }
    if (message.span?.length) {
      obj.span = message.span.map((e) => Span.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TokenClassValue>, I>>(base?: I): TokenClassValue {
    return TokenClassValue.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TokenClassValue>, I>>(object: I): TokenClassValue {
    const message = createBaseTokenClassValue();
    message.token = object.token ?? "";
    message.label = object.label?.map((e) => Classification.fromPartial(e)) || [];
    message.span = object.span?.map((e) => Span.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTokenClassSequence(): TokenClassSequence {
  return { results: [] };
}

export const TokenClassSequence: MessageFns<TokenClassSequence> = {
  encode(message: TokenClassSequence, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      TokenClassValue.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TokenClassSequence {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTokenClassSequence();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(TokenClassValue.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TokenClassSequence {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => TokenClassValue.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TokenClassSequence): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => TokenClassValue.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TokenClassSequence>, I>>(base?: I): TokenClassSequence {
    return TokenClassSequence.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TokenClassSequence>, I>>(object: I): TokenClassSequence {
    const message = createBaseTokenClassSequence();
    message.results = object.results?.map((e) => TokenClassValue.fromPartial(e)) || [];
    return message;
  },
};

function createBaseTokenClassResponse(): TokenClassResponse {
  return { results: [] };
}

export const TokenClassResponse: MessageFns<TokenClassResponse> = {
  encode(message: TokenClassResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      TokenClassSequence.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): TokenClassResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTokenClassResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(TokenClassSequence.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): TokenClassResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => TokenClassSequence.fromJSON(e))
        : [],
    };
  },

  toJSON(message: TokenClassResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => TokenClassSequence.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<TokenClassResponse>, I>>(base?: I): TokenClassResponse {
    return TokenClassResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<TokenClassResponse>, I>>(object: I): TokenClassResponse {
    const message = createBaseTokenClassResponse();
    message.results = object.results?.map((e) => TokenClassSequence.fromPartial(e)) || [];
    return message;
  },
};

function createBaseAnalyzeIntentContext(): AnalyzeIntentContext {
  return {};
}

export const AnalyzeIntentContext: MessageFns<AnalyzeIntentContext> = {
  encode(_: AnalyzeIntentContext, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnalyzeIntentContext {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnalyzeIntentContext();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(_: any): AnalyzeIntentContext {
    return {};
  },

  toJSON(_: AnalyzeIntentContext): unknown {
    const obj: any = {};
    return obj;
  },

  create<I extends Exact<DeepPartial<AnalyzeIntentContext>, I>>(base?: I): AnalyzeIntentContext {
    return AnalyzeIntentContext.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AnalyzeIntentContext>, I>>(_: I): AnalyzeIntentContext {
    const message = createBaseAnalyzeIntentContext();
    return message;
  },
};

function createBaseAnalyzeIntentOptions(): AnalyzeIntentOptions {
  return { previousIntent: undefined, vectors: undefined, domain: "", lang: "" };
}

export const AnalyzeIntentOptions: MessageFns<AnalyzeIntentOptions> = {
  encode(message: AnalyzeIntentOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.previousIntent !== undefined) {
      writer.uint32(10).string(message.previousIntent);
    }
    if (message.vectors !== undefined) {
      AnalyzeIntentContext.encode(message.vectors, writer.uint32(18).fork()).join();
    }
    if (message.domain !== "") {
      writer.uint32(26).string(message.domain);
    }
    if (message.lang !== "") {
      writer.uint32(34).string(message.lang);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnalyzeIntentOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnalyzeIntentOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.previousIntent = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.vectors = AnalyzeIntentContext.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.domain = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.lang = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnalyzeIntentOptions {
    return {
      previousIntent: isSet(object.previousIntent) ? globalThis.String(object.previousIntent) : undefined,
      vectors: isSet(object.vectors) ? AnalyzeIntentContext.fromJSON(object.vectors) : undefined,
      domain: isSet(object.domain) ? globalThis.String(object.domain) : "",
      lang: isSet(object.lang) ? globalThis.String(object.lang) : "",
    };
  },

  toJSON(message: AnalyzeIntentOptions): unknown {
    const obj: any = {};
    if (message.previousIntent !== undefined) {
      obj.previousIntent = message.previousIntent;
    }
    if (message.vectors !== undefined) {
      obj.vectors = AnalyzeIntentContext.toJSON(message.vectors);
    }
    if (message.domain !== "") {
      obj.domain = message.domain;
    }
    if (message.lang !== "") {
      obj.lang = message.lang;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AnalyzeIntentOptions>, I>>(base?: I): AnalyzeIntentOptions {
    return AnalyzeIntentOptions.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AnalyzeIntentOptions>, I>>(object: I): AnalyzeIntentOptions {
    const message = createBaseAnalyzeIntentOptions();
    message.previousIntent = object.previousIntent ?? undefined;
    message.vectors = (object.vectors !== undefined && object.vectors !== null)
      ? AnalyzeIntentContext.fromPartial(object.vectors)
      : undefined;
    message.domain = object.domain ?? "";
    message.lang = object.lang ?? "";
    return message;
  },
};

function createBaseAnalyzeIntentRequest(): AnalyzeIntentRequest {
  return { query: "", options: undefined };
}

export const AnalyzeIntentRequest: MessageFns<AnalyzeIntentRequest> = {
  encode(message: AnalyzeIntentRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.query !== "") {
      writer.uint32(10).string(message.query);
    }
    if (message.options !== undefined) {
      AnalyzeIntentOptions.encode(message.options, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnalyzeIntentRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnalyzeIntentRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.query = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.options = AnalyzeIntentOptions.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnalyzeIntentRequest {
    return {
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      options: isSet(object.options) ? AnalyzeIntentOptions.fromJSON(object.options) : undefined,
    };
  },

  toJSON(message: AnalyzeIntentRequest): unknown {
    const obj: any = {};
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.options !== undefined) {
      obj.options = AnalyzeIntentOptions.toJSON(message.options);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AnalyzeIntentRequest>, I>>(base?: I): AnalyzeIntentRequest {
    return AnalyzeIntentRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AnalyzeIntentRequest>, I>>(object: I): AnalyzeIntentRequest {
    const message = createBaseAnalyzeIntentRequest();
    message.query = object.query ?? "";
    message.options = (object.options !== undefined && object.options !== null)
      ? AnalyzeIntentOptions.fromPartial(object.options)
      : undefined;
    return message;
  },
};

function createBaseAnalyzeIntentResponse(): AnalyzeIntentResponse {
  return { intent: undefined, slots: [], domainStr: "", domain: undefined };
}

export const AnalyzeIntentResponse: MessageFns<AnalyzeIntentResponse> = {
  encode(message: AnalyzeIntentResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.intent !== undefined) {
      Classification.encode(message.intent, writer.uint32(10).fork()).join();
    }
    for (const v of message.slots) {
      TokenClassValue.encode(v!, writer.uint32(18).fork()).join();
    }
    if (message.domainStr !== "") {
      writer.uint32(26).string(message.domainStr);
    }
    if (message.domain !== undefined) {
      Classification.encode(message.domain, writer.uint32(34).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnalyzeIntentResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnalyzeIntentResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.intent = Classification.decode(reader, reader.uint32());
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.slots.push(TokenClassValue.decode(reader, reader.uint32()));
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.domainStr = reader.string();
          continue;
        }
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.domain = Classification.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnalyzeIntentResponse {
    return {
      intent: isSet(object.intent) ? Classification.fromJSON(object.intent) : undefined,
      slots: globalThis.Array.isArray(object?.slots) ? object.slots.map((e: any) => TokenClassValue.fromJSON(e)) : [],
      domainStr: isSet(object.domainStr) ? globalThis.String(object.domainStr) : "",
      domain: isSet(object.domain) ? Classification.fromJSON(object.domain) : undefined,
    };
  },

  toJSON(message: AnalyzeIntentResponse): unknown {
    const obj: any = {};
    if (message.intent !== undefined) {
      obj.intent = Classification.toJSON(message.intent);
    }
    if (message.slots?.length) {
      obj.slots = message.slots.map((e) => TokenClassValue.toJSON(e));
    }
    if (message.domainStr !== "") {
      obj.domainStr = message.domainStr;
    }
    if (message.domain !== undefined) {
      obj.domain = Classification.toJSON(message.domain);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AnalyzeIntentResponse>, I>>(base?: I): AnalyzeIntentResponse {
    return AnalyzeIntentResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AnalyzeIntentResponse>, I>>(object: I): AnalyzeIntentResponse {
    const message = createBaseAnalyzeIntentResponse();
    message.intent = (object.intent !== undefined && object.intent !== null)
      ? Classification.fromPartial(object.intent)
      : undefined;
    message.slots = object.slots?.map((e) => TokenClassValue.fromPartial(e)) || [];
    message.domainStr = object.domainStr ?? "";
    message.domain = (object.domain !== undefined && object.domain !== null)
      ? Classification.fromPartial(object.domain)
      : undefined;
    return message;
  },
};

function createBaseAnalyzeEntitiesOptions(): AnalyzeEntitiesOptions {
  return { lang: "" };
}

export const AnalyzeEntitiesOptions: MessageFns<AnalyzeEntitiesOptions> = {
  encode(message: AnalyzeEntitiesOptions, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.lang !== "") {
      writer.uint32(34).string(message.lang);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnalyzeEntitiesOptions {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnalyzeEntitiesOptions();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 4: {
          if (tag !== 34) {
            break;
          }

          message.lang = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnalyzeEntitiesOptions {
    return { lang: isSet(object.lang) ? globalThis.String(object.lang) : "" };
  },

  toJSON(message: AnalyzeEntitiesOptions): unknown {
    const obj: any = {};
    if (message.lang !== "") {
      obj.lang = message.lang;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AnalyzeEntitiesOptions>, I>>(base?: I): AnalyzeEntitiesOptions {
    return AnalyzeEntitiesOptions.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AnalyzeEntitiesOptions>, I>>(object: I): AnalyzeEntitiesOptions {
    const message = createBaseAnalyzeEntitiesOptions();
    message.lang = object.lang ?? "";
    return message;
  },
};

function createBaseAnalyzeEntitiesRequest(): AnalyzeEntitiesRequest {
  return { query: "", options: undefined };
}

export const AnalyzeEntitiesRequest: MessageFns<AnalyzeEntitiesRequest> = {
  encode(message: AnalyzeEntitiesRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.query !== "") {
      writer.uint32(10).string(message.query);
    }
    if (message.options !== undefined) {
      AnalyzeEntitiesOptions.encode(message.options, writer.uint32(18).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): AnalyzeEntitiesRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseAnalyzeEntitiesRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.query = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.options = AnalyzeEntitiesOptions.decode(reader, reader.uint32());
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): AnalyzeEntitiesRequest {
    return {
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      options: isSet(object.options) ? AnalyzeEntitiesOptions.fromJSON(object.options) : undefined,
    };
  },

  toJSON(message: AnalyzeEntitiesRequest): unknown {
    const obj: any = {};
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.options !== undefined) {
      obj.options = AnalyzeEntitiesOptions.toJSON(message.options);
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<AnalyzeEntitiesRequest>, I>>(base?: I): AnalyzeEntitiesRequest {
    return AnalyzeEntitiesRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<AnalyzeEntitiesRequest>, I>>(object: I): AnalyzeEntitiesRequest {
    const message = createBaseAnalyzeEntitiesRequest();
    message.query = object.query ?? "";
    message.options = (object.options !== undefined && object.options !== null)
      ? AnalyzeEntitiesOptions.fromPartial(object.options)
      : undefined;
    return message;
  },
};

function createBaseNaturalQueryRequest(): NaturalQueryRequest {
  return { query: "", topN: 0, context: "" };
}

export const NaturalQueryRequest: MessageFns<NaturalQueryRequest> = {
  encode(message: NaturalQueryRequest, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.query !== "") {
      writer.uint32(10).string(message.query);
    }
    if (message.topN !== 0) {
      writer.uint32(16).uint32(message.topN);
    }
    if (message.context !== "") {
      writer.uint32(26).string(message.context);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NaturalQueryRequest {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNaturalQueryRequest();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.query = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.topN = reader.uint32();
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.context = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NaturalQueryRequest {
    return {
      query: isSet(object.query) ? globalThis.String(object.query) : "",
      topN: isSet(object.topN) ? globalThis.Number(object.topN) : 0,
      context: isSet(object.context) ? globalThis.String(object.context) : "",
    };
  },

  toJSON(message: NaturalQueryRequest): unknown {
    const obj: any = {};
    if (message.query !== "") {
      obj.query = message.query;
    }
    if (message.topN !== 0) {
      obj.topN = Math.round(message.topN);
    }
    if (message.context !== "") {
      obj.context = message.context;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<NaturalQueryRequest>, I>>(base?: I): NaturalQueryRequest {
    return NaturalQueryRequest.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<NaturalQueryRequest>, I>>(object: I): NaturalQueryRequest {
    const message = createBaseNaturalQueryRequest();
    message.query = object.query ?? "";
    message.topN = object.topN ?? 0;
    message.context = object.context ?? "";
    return message;
  },
};

function createBaseNaturalQueryResult(): NaturalQueryResult {
  return { answer: "", score: 0 };
}

export const NaturalQueryResult: MessageFns<NaturalQueryResult> = {
  encode(message: NaturalQueryResult, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.answer !== "") {
      writer.uint32(10).string(message.answer);
    }
    if (message.score !== 0) {
      writer.uint32(21).float(message.score);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NaturalQueryResult {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNaturalQueryResult();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.answer = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 21) {
            break;
          }

          message.score = reader.float();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NaturalQueryResult {
    return {
      answer: isSet(object.answer) ? globalThis.String(object.answer) : "",
      score: isSet(object.score) ? globalThis.Number(object.score) : 0,
    };
  },

  toJSON(message: NaturalQueryResult): unknown {
    const obj: any = {};
    if (message.answer !== "") {
      obj.answer = message.answer;
    }
    if (message.score !== 0) {
      obj.score = message.score;
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<NaturalQueryResult>, I>>(base?: I): NaturalQueryResult {
    return NaturalQueryResult.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<NaturalQueryResult>, I>>(object: I): NaturalQueryResult {
    const message = createBaseNaturalQueryResult();
    message.answer = object.answer ?? "";
    message.score = object.score ?? 0;
    return message;
  },
};

function createBaseNaturalQueryResponse(): NaturalQueryResponse {
  return { results: [] };
}

export const NaturalQueryResponse: MessageFns<NaturalQueryResponse> = {
  encode(message: NaturalQueryResponse, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    for (const v of message.results) {
      NaturalQueryResult.encode(v!, writer.uint32(10).fork()).join();
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): NaturalQueryResponse {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseNaturalQueryResponse();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.results.push(NaturalQueryResult.decode(reader, reader.uint32()));
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  fromJSON(object: any): NaturalQueryResponse {
    return {
      results: globalThis.Array.isArray(object?.results)
        ? object.results.map((e: any) => NaturalQueryResult.fromJSON(e))
        : [],
    };
  },

  toJSON(message: NaturalQueryResponse): unknown {
    const obj: any = {};
    if (message.results?.length) {
      obj.results = message.results.map((e) => NaturalQueryResult.toJSON(e));
    }
    return obj;
  },

  create<I extends Exact<DeepPartial<NaturalQueryResponse>, I>>(base?: I): NaturalQueryResponse {
    return NaturalQueryResponse.fromPartial(base ?? ({} as any));
  },
  fromPartial<I extends Exact<DeepPartial<NaturalQueryResponse>, I>>(object: I): NaturalQueryResponse {
    const message = createBaseNaturalQueryResponse();
    message.results = object.results?.map((e) => NaturalQueryResult.fromPartial(e)) || [];
    return message;
  },
};

export type RivaLanguageUnderstandingService = typeof RivaLanguageUnderstandingService;
export const RivaLanguageUnderstandingService = {
  /**
   * ClassifyText takes as input an input/query string and parameters related
   * to the requested model to use to evaluate the text. The service evaluates the
   * text with the requested model, and returns one or more classifications.
   */
  classifyText: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyText",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: TextClassRequest): Buffer => Buffer.from(TextClassRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): TextClassRequest => TextClassRequest.decode(value),
    responseSerialize: (value: TextClassResponse): Buffer => Buffer.from(TextClassResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): TextClassResponse => TextClassResponse.decode(value),
  },
  /**
   * ClassifyTokens takes as input either a string or list of tokens and parameters
   * related to which model to use. The service evaluates the text with the requested
   * model, performing additional tokenization if necessary, and returns one or more
   * class labels per token.
   */
  classifyTokens: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/ClassifyTokens",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: TokenClassRequest): Buffer => Buffer.from(TokenClassRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): TokenClassRequest => TokenClassRequest.decode(value),
    responseSerialize: (value: TokenClassResponse): Buffer => Buffer.from(TokenClassResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): TokenClassResponse => TokenClassResponse.decode(value),
  },
  /**
   * TransformText takes an input/query string and parameters related to the
   * requested model and returns another string. The behavior of the function
   * is defined entirely by the underlying model and may be used for
   * tasks like translation, adding punctuation, augment the input directly, etc.
   */
  transformText: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/TransformText",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: TextTransformRequest): Buffer => Buffer.from(TextTransformRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): TextTransformRequest => TextTransformRequest.decode(value),
    responseSerialize: (value: TextTransformResponse): Buffer =>
      Buffer.from(TextTransformResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): TextTransformResponse => TextTransformResponse.decode(value),
  },
  /**
   * AnalyzeEntities accepts an input string and returns all named entities within
   * the text, as well as a category and likelihood.
   */
  analyzeEntities: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/AnalyzeEntities",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: AnalyzeEntitiesRequest): Buffer =>
      Buffer.from(AnalyzeEntitiesRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): AnalyzeEntitiesRequest => AnalyzeEntitiesRequest.decode(value),
    responseSerialize: (value: TokenClassResponse): Buffer => Buffer.from(TokenClassResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): TokenClassResponse => TokenClassResponse.decode(value),
  },
  /**
   * AnalyzeIntent accepts an input string and returns the most likely
   * intent as well as slots relevant to that intent.
   *
   * The model requires that a valid "domain" be passed in, and optionally
   * supports including a previous intent classification result to provide
   * context for the model.
   */
  analyzeIntent: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/AnalyzeIntent",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: AnalyzeIntentRequest): Buffer => Buffer.from(AnalyzeIntentRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): AnalyzeIntentRequest => AnalyzeIntentRequest.decode(value),
    responseSerialize: (value: AnalyzeIntentResponse): Buffer =>
      Buffer.from(AnalyzeIntentResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): AnalyzeIntentResponse => AnalyzeIntentResponse.decode(value),
  },
  /**
   * PunctuateText takes text with no- or limited- punctuation and returns
   * the same text with corrected punctuation and capitalization.
   */
  punctuateText: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/PunctuateText",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: TextTransformRequest): Buffer => Buffer.from(TextTransformRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): TextTransformRequest => TextTransformRequest.decode(value),
    responseSerialize: (value: TextTransformResponse): Buffer =>
      Buffer.from(TextTransformResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): TextTransformResponse => TextTransformResponse.decode(value),
  },
  /**
   * NaturalQuery is a search function that enables querying one or more documents
   * or contexts with a query that is written in natural language.
   */
  naturalQuery: {
    path: "/nvidia.riva.nlp.RivaLanguageUnderstanding/NaturalQuery",
    requestStream: false,
    responseStream: false,
    requestSerialize: (value: NaturalQueryRequest): Buffer => Buffer.from(NaturalQueryRequest.encode(value).finish()),
    requestDeserialize: (value: Buffer): NaturalQueryRequest => NaturalQueryRequest.decode(value),
    responseSerialize: (value: NaturalQueryResponse): Buffer =>
      Buffer.from(NaturalQueryResponse.encode(value).finish()),
    responseDeserialize: (value: Buffer): NaturalQueryResponse => NaturalQueryResponse.decode(value),
  },
} as const;

export interface RivaLanguageUnderstandingServer extends UntypedServiceImplementation {
  /**
   * ClassifyText takes as input an input/query string and parameters related
   * to the requested model to use to evaluate the text. The service evaluates the
   * text with the requested model, and returns one or more classifications.
   */
  classifyText: handleUnaryCall<TextClassRequest, TextClassResponse>;
  /**
   * ClassifyTokens takes as input either a string or list of tokens and parameters
   * related to which model to use. The service evaluates the text with the requested
   * model, performing additional tokenization if necessary, and returns one or more
   * class labels per token.
   */
  classifyTokens: handleUnaryCall<TokenClassRequest, TokenClassResponse>;
  /**
   * TransformText takes an input/query string and parameters related to the
   * requested model and returns another string. The behavior of the function
   * is defined entirely by the underlying model and may be used for
   * tasks like translation, adding punctuation, augment the input directly, etc.
   */
  transformText: handleUnaryCall<TextTransformRequest, TextTransformResponse>;
  /**
   * AnalyzeEntities accepts an input string and returns all named entities within
   * the text, as well as a category and likelihood.
   */
  analyzeEntities: handleUnaryCall<AnalyzeEntitiesRequest, TokenClassResponse>;
  /**
   * AnalyzeIntent accepts an input string and returns the most likely
   * intent as well as slots relevant to that intent.
   *
   * The model requires that a valid "domain" be passed in, and optionally
   * supports including a previous intent classification result to provide
   * context for the model.
   */
  analyzeIntent: handleUnaryCall<AnalyzeIntentRequest, AnalyzeIntentResponse>;
  /**
   * PunctuateText takes text with no- or limited- punctuation and returns
   * the same text with corrected punctuation and capitalization.
   */
  punctuateText: handleUnaryCall<TextTransformRequest, TextTransformResponse>;
  /**
   * NaturalQuery is a search function that enables querying one or more documents
   * or contexts with a query that is written in natural language.
   */
  naturalQuery: handleUnaryCall<NaturalQueryRequest, NaturalQueryResponse>;
}

export interface RivaLanguageUnderstandingClient extends Client {
  /**
   * ClassifyText takes as input an input/query string and parameters related
   * to the requested model to use to evaluate the text. The service evaluates the
   * text with the requested model, and returns one or more classifications.
   */
  classifyText(
    request: TextClassRequest,
    callback: (error: ServiceError | null, response: TextClassResponse) => void,
  ): ClientUnaryCall;
  classifyText(
    request: TextClassRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TextClassResponse) => void,
  ): ClientUnaryCall;
  classifyText(
    request: TextClassRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TextClassResponse) => void,
  ): ClientUnaryCall;
  /**
   * ClassifyTokens takes as input either a string or list of tokens and parameters
   * related to which model to use. The service evaluates the text with the requested
   * model, performing additional tokenization if necessary, and returns one or more
   * class labels per token.
   */
  classifyTokens(
    request: TokenClassRequest,
    callback: (error: ServiceError | null, response: TokenClassResponse) => void,
  ): ClientUnaryCall;
  classifyTokens(
    request: TokenClassRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TokenClassResponse) => void,
  ): ClientUnaryCall;
  classifyTokens(
    request: TokenClassRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TokenClassResponse) => void,
  ): ClientUnaryCall;
  /**
   * TransformText takes an input/query string and parameters related to the
   * requested model and returns another string. The behavior of the function
   * is defined entirely by the underlying model and may be used for
   * tasks like translation, adding punctuation, augment the input directly, etc.
   */
  transformText(
    request: TextTransformRequest,
    callback: (error: ServiceError | null, response: TextTransformResponse) => void,
  ): ClientUnaryCall;
  transformText(
    request: TextTransformRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TextTransformResponse) => void,
  ): ClientUnaryCall;
  transformText(
    request: TextTransformRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TextTransformResponse) => void,
  ): ClientUnaryCall;
  /**
   * AnalyzeEntities accepts an input string and returns all named entities within
   * the text, as well as a category and likelihood.
   */
  analyzeEntities(
    request: AnalyzeEntitiesRequest,
    callback: (error: ServiceError | null, response: TokenClassResponse) => void,
  ): ClientUnaryCall;
  analyzeEntities(
    request: AnalyzeEntitiesRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TokenClassResponse) => void,
  ): ClientUnaryCall;
  analyzeEntities(
    request: AnalyzeEntitiesRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TokenClassResponse) => void,
  ): ClientUnaryCall;
  /**
   * AnalyzeIntent accepts an input string and returns the most likely
   * intent as well as slots relevant to that intent.
   *
   * The model requires that a valid "domain" be passed in, and optionally
   * supports including a previous intent classification result to provide
   * context for the model.
   */
  analyzeIntent(
    request: AnalyzeIntentRequest,
    callback: (error: ServiceError | null, response: AnalyzeIntentResponse) => void,
  ): ClientUnaryCall;
  analyzeIntent(
    request: AnalyzeIntentRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: AnalyzeIntentResponse) => void,
  ): ClientUnaryCall;
  analyzeIntent(
    request: AnalyzeIntentRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: AnalyzeIntentResponse) => void,
  ): ClientUnaryCall;
  /**
   * PunctuateText takes text with no- or limited- punctuation and returns
   * the same text with corrected punctuation and capitalization.
   */
  punctuateText(
    request: TextTransformRequest,
    callback: (error: ServiceError | null, response: TextTransformResponse) => void,
  ): ClientUnaryCall;
  punctuateText(
    request: TextTransformRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: TextTransformResponse) => void,
  ): ClientUnaryCall;
  punctuateText(
    request: TextTransformRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: TextTransformResponse) => void,
  ): ClientUnaryCall;
  /**
   * NaturalQuery is a search function that enables querying one or more documents
   * or contexts with a query that is written in natural language.
   */
  naturalQuery(
    request: NaturalQueryRequest,
    callback: (error: ServiceError | null, response: NaturalQueryResponse) => void,
  ): ClientUnaryCall;
  naturalQuery(
    request: NaturalQueryRequest,
    metadata: Metadata,
    callback: (error: ServiceError | null, response: NaturalQueryResponse) => void,
  ): ClientUnaryCall;
  naturalQuery(
    request: NaturalQueryRequest,
    metadata: Metadata,
    options: Partial<CallOptions>,
    callback: (error: ServiceError | null, response: NaturalQueryResponse) => void,
  ): ClientUnaryCall;
}

export const RivaLanguageUnderstandingClient = makeGenericClientConstructor(
  RivaLanguageUnderstandingService,
  "nvidia.riva.nlp.RivaLanguageUnderstanding",
) as unknown as {
  new (
    address: string,
    credentials: ChannelCredentials,
    options?: Partial<ClientOptions>,
  ): RivaLanguageUnderstandingClient;
  service: typeof RivaLanguageUnderstandingService;
  serviceName: string;
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

type KeysOfUnion<T> = T extends T ? keyof T : never;
export type Exact<P, I extends P> = P extends Builtin ? P
  : P & { [K in keyof P]: Exact<P[K], I[K]> } & { [K in Exclude<keyof I, KeysOfUnion<P>>]: never };

function isSet(value: any): boolean {
  return value !== null && value !== undefined;
}

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  fromJSON(object: any): T;
  toJSON(message: T): unknown;
  create<I extends Exact<DeepPartial<T>, I>>(base?: I): T;
  fromPartial<I extends Exact<DeepPartial<T>, I>>(object: I): T;
}
