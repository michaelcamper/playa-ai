{
  "id": "40bf8f973c2a799836d6371d24b414b1c4f1dcbf90ff694bd76be1578259bfc2",
  "url": "https://journal.burningman.org/2018/02/philosophical-center/the-theme/from-robots-to-intelligent-autonomous-agents/",
  "title": "From Robots to Intelligent, Autonomous Agents",
  "author": "Nicole Radziwill",
  "dateISO": "2018-02-27T15:07:25-08:00",
  "content": "“I got you $50 off your monthly internet bill!” she chirped, as I walked in from work. I hadn’t even closed the door yet.\n\n“That’s great! How did you do it?” I asked. It was nice to be greeted with good news.\n\n“Easy,” she replied. “Your mobile phone service provider was having a promotion, and they needed your social media data for the last 30 days. Since you told me you wouldn’t mind if I shared it, I went ahead and signed you up.”\n\nMaybe these robots aren’t so bad, I thought. I never wanted one of these voice-activated home assistants, but if they can save me money, I guess I’ll adapt.\n\nThe concept of a “robot” emerged after World War I. The word, from the Czech “roboti” — meaning forced labor — was first used in Czech writer Karel Čapek’s (cha-peck) 1920 play called R.U.R. (Rossum’s Universal Robots). This story about assembled biological beings, like the hosts in Westworld or the Cylons in Battlestar Galactica, was sufficiently compelling to earn a spot on Broadway and in theatres from London to Los Angeles. Adaptations for film and radio appeared through the 1930’s and 1940’s. A live performance featuring real robots as actors was even performed in Prague in 2015.\n\nIn the play, Čapek’s robots eventually take over the world, but can’t figure out how to reproduce. In the end, Alquist (the last human left on Earth) senses that two of the robots are falling in love, something they weren’t programmed to do. The play closes as Alquist wistfully “hopes for the best” and the audience, no doubt, rejoices that something like this couldn’t possibly happen in real life.\n\nRobots are designed to be agents — machines that will do things on your behalf, sometimes even making decisions and taking actions in a way that (hopefully) you would. Although we usually think of robots as hardware (like surgical robots that handle precision cutting, or industrial robots that assemble and transform parts in sometimes dangerous environments), they can be software too (although in that case, many people instinctively shorten the term to “bot”). Artificial intelligence and machine learning techniques can be used by either of these to acquire and process data, make complex decisions, and take actions towards specific goals.\n\n[Spoiler: Our home assistant hasn’t actually saved us money on our internet service, and to be honest, I’m not very fond of it yet. First, I’d rather a text-based interface than voice-based — my phone and my computer satisfy all my needs (at least all the needs I know I have). Also, it creeps me out to have a device listening to me all the time, even though I know my phone does the same thing. But if I could train the home assistant with my preferences, and it could work intelligently on my behalf to make doctor’s appointments, order new checks, or cancel those domain names that I’ll never use, ordered late on Saturday-nights-of-great-ideas… I’d probably change my mind… and fast.]\n\nBut what happens when our things become more autonomous, or can better represent (and act on) our thoughts, beliefs, emotions, and intentions? What happens when non-humans are granted rights? The Saudi robot Sophia has already been granted citizenship there, and in the US, the notion of “corporate personhood” (though controversial) sets the stage for machine personhood. If groups of people can be treated like individuals, why not machines that act as agents of the people?\n\nIn 2010, the US Supreme Court (in Citizens United vs FEC) decided that “non-persons” have the right to free speech. Because of this right, corporations would finally be able to buy political ads to support (or oppose) political candidates. Turns out, the First Amendment protects not only the entity’s right to share information — but also to receive it. The listener has rights that are separate and distinct from those of the speaker, and the listener also has a right to know who the message is coming from. The role of the government is specific: to protect non-consenting individuals so you don’t have to get a person’s (or company’s) free speech forced on you.\n\nIf your autonomous agent has a right to free speech and you own the agent, is that right independent of yours, or an extension of yours? How can you keep your stuff from ganging up on you, or from revealing secrets that you hadn’t previously recognized were secret? Questions like this make me happy I chose data science over law school.\n\nPanpsychism is the philosophy that all things — even atoms, and books, and toasters — have some sort of mind or consciousness. The exact meaning of these terms has been the subject of intense philosophical debate for centuries, and panpsychism itself been widely shrugged off as “crazy” or “unrealistic.” But if you examine the behavior of objects rather than their intrinsic nature, the concept of panpsychism fits — and shouldn’t be completely discounted when designing intelligent objects that will be situated in intelligent environments. For example, have you ever been engaged in an argument with a bot on Twitter, only to realize later that it was a non-human entity stirring your emotions? Even if the technologies themselves don’t have a mind or consciousness, they can clearly interfere with the health and sanity of yours.\n\nYour toaster may never have an inner life, but it will be able to interact with you and your other belongings, and accomplish tasks, exchange information, or engage in transactions on your behalf. It may even be able to interact with other people’s stuff. In fact, the Iota (or “Internet of Things Application”) cryptocurrency was originally designed so that future objects have a value store they can use to engage in transactions with one another.\n\nWhen you can’t tell if you’re interacting with a human or non-human online, does the distinction even matter any more? When IoT objects around us start demonstrating “free will” — or at least making decisions that are unexpected (and maybe also opaque) to us — can we continue thinking about objects as being solely passive?\n\nThe answer to both questions is probably not. But we don’t have to buy in to the notion that “everything has a mind or consciousness” to thrive in an intelligent environment either. We can, however, use that idea as a lens to examine how to broaden and enrich our community’s ethos and culture in a hybrid society of interoperating people and machines.\n\nSome of the 10 Principles can help start discussions about how to ethically integrate IoT and other intelligent technologies into our culture:\n\nOver the next decade or two, as our lives become even more technologically steeped than they are now, we will be exploring the “use cases” and “abuse cases” of intelligent technologies together. Čapek’s robots may have been science fiction, but today’s intelligent agents are not — and the health of their relationships may reflect how well we address trust, power, control, and consent within ours.\n\nCover image:  Intersection by James Reagent, Charles Fields, and company (Photo by THESSY)",
  "source": "burningman_journal",
  "category": "philosophical-center",
  "topic": "the-theme"
}